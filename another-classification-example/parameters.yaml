# Model
model:
  activation: relu
  dense_units: 128
  dropout: 0.19

# Optimizer
optim_params:
  optimizer: Adam # SGD, Nadam
  init_learning_rate: 0.0047

# Training
training:
  batch_size: 64
  early_stopping: 20
  n_epochs: 100
  shuffle: True
  save_every: 20
