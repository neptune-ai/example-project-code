# Model
model:
  activation: relu
  dense_units: 128
  dropout: 0.11

# Optimizer
optim_params:
  optimizer: SGD # Adam, Nadam
  init_learning_rate: 0.005

# Training
training:
  batch_size: 64
  early_stopping: 20
  n_epochs: 100
  shuffle: True
  save_every: 20
