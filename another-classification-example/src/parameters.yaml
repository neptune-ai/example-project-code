# Model
model:
  activation: elu
  dense_units: 64
  dropout: 0.2

# Optimizer
optim_params:
  optimizer: Adam # Nadam, SGD
  init_learning_rate: 0.003

# Training
training:
  batch_size: 64
  early_stopping: 10
  n_epochs: 100
  shuffle: True
  save_every: 20
