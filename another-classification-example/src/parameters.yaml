# Model
model:
  activation: elu
  dense_units: 128
  dropout: 0.2

# Optimizer
optim_params:
  optimizer: Adam
  init_learning_rate: 0.001

# Training
training:
  batch_size: 64
  early_stopping: 100
  n_epochs: 100
  shuffle: True
  save_every: 20
  
